{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction and Vectorization of documents extracted from structured and unstructured PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNIFI Value Frameworks PDF Lifting Competition\n",
    "- The objective of the competition is to create a solution that parses annual reports in PDF format and extracts information about pre-defined activity metrics, in order for UNIFI to obtain specific information about sustainability at a given company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install tesseract-ocr -y\n",
    "!sudo apt install libtesseract-dev -y\n",
    "!sudo apt-get install poppler-utils -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain unstructured[all-docs] pydantic lxml openai tiktoken opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install qdrant_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os\n",
    "import uuid\n",
    "import base64\n",
    "import torch\n",
    "from IPython import display\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "from langchain.schema.document import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env file within your directory and add your OpenAI API key and ANTHROPIC API key as OPENAI_API_KEY and ANTHROPIC_API_KEY respectively\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "anthropic_api_key = os.environ['ANTHROPIC_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(api_key=anthropic_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"gpt-4-turbo-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load OpenAI's new text-embedding-3-large embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use partition_pdf function from unstructured to extract text, table, and image elements from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./pdfs_images_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elements_from_document(file_path):\n",
    "  image_path = f\"{output_path}/{os.path.splitext(os.path.basename(file_path))[0]}\"\n",
    "  raw_pdf_elements = partition_pdf(\n",
    "    filename=file_path,\n",
    "    strategy=\"hi_res\",\n",
    "    extract_images_in_pdf=True,\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    extract_image_block_to_payload=False,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    extract_image_block_output_dir=image_path,\n",
    "  )\n",
    "  return raw_pdf_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_table_summaries(raw_pdf_elements):\n",
    "  # Get text summaries and table summaries\n",
    "  text_elements = []\n",
    "  table_elements = []\n",
    "\n",
    "  text_summaries = []\n",
    "  table_summaries = []\n",
    "\n",
    "  summary_prompt = \"\"\"\n",
    "  Summarize the following ensuring you note the numbers and percentages of the various metrics listed {element_type}:\n",
    "  {element}\n",
    "  \"\"\"\n",
    "  summary_chain = LLMChain(\n",
    "      llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3),\n",
    "      prompt=PromptTemplate.from_template(summary_prompt)\n",
    "  )\n",
    "\n",
    "  for e in raw_pdf_elements:\n",
    "      if 'CompositeElement' in repr(e):\n",
    "          text_elements.append(e.text)\n",
    "          summary = summary_chain.run({'element_type': 'text', 'element': e})\n",
    "          text_summaries.append(summary)\n",
    "\n",
    "      elif 'Table' in repr(e):\n",
    "          table_elements.append(e.text)\n",
    "          summary = summary_chain.run({'element_type': 'table', 'element': e})\n",
    "          table_summaries.append(summary)\n",
    "  return text_elements, text_summaries, table_elements, table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "def summarize_image(encoded_image):\n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"data\": encoded_image,\n",
    "                                \"media_type\": \"image/jpeg\"  # Add the media_type field with appropriate value\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"Can you describe the image and extract all activity metrics which are numbers and percentages that represent various metrics for various companies from the image?\"\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        text_blocks = [content_block.text for content_block in message.content if content_block.type == 'text']\n",
    "        text = text_blocks[0]\n",
    "        return text\n",
    "    except anthropic.APIConnectionError as e:\n",
    "        print(\"The server could not be reached\")\n",
    "        print(e.__cause__)  # an underlying Exception, likely raised within httpx.\n",
    "        prompt = [\n",
    "            SystemMessage(content=\"You are a bot that is good at analyzing images in PDFs.\"),\n",
    "            HumanMessage(content=[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Can you describe the image and extract all activity metrics which are numbers and percentages that represent various metrics for various companies from the image?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                    },\n",
    "                },\n",
    "            ])\n",
    "        ]\n",
    "        try:\n",
    "            response = ChatOpenAI(model=\"gpt-4-vision-preview\", openai_api_key=openai.api_key, max_tokens=1024).invoke(prompt)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error message: {e}\")\n",
    "            return \"Image could not be summarized.\"\n",
    "    except anthropic.RateLimitError as e:\n",
    "        print(\"A 429 status code was received; we should back off a bit.\")\n",
    "        prompt = [\n",
    "            SystemMessage(content=\"You are a bot that is good at analyzing images in PDFs.\"),\n",
    "            HumanMessage(content=[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Can you describe the image and extract all activity metrics which are numbers and percentages that represent various metrics for various companies from the image?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                    },\n",
    "                },\n",
    "            ])\n",
    "        ]\n",
    "        try:\n",
    "            response = ChatOpenAI(model=\"gpt-4-vision-preview\", openai_api_key=openai.api_key, max_tokens=1024).invoke(prompt)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error message: {e}\")\n",
    "            return \"Image could not be summarized.\"\n",
    "    except anthropic.APIStatusError as e:\n",
    "        print(\"Another non-200-range status code was received\")\n",
    "        print(e.status_code)\n",
    "        print(e.response)\n",
    "        prompt = [\n",
    "            SystemMessage(content=\"You are a bot that is good at analyzing images in PDFs.\"),\n",
    "            HumanMessage(content=[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Can you describe the image and extract all activity metrics which are numbers and percentages that represent various metrics for various companies from the image?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                    },\n",
    "                },\n",
    "            ])\n",
    "        ]\n",
    "        try:\n",
    "            response = ChatOpenAI(model=\"gpt-4-vision-preview\", openai_api_key=openai.api_key, max_tokens=1024).invoke(prompt)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error message: {e}\")\n",
    "            return \"Image could not be summarized.\"\n",
    "    \n",
    "\n",
    "\n",
    "def get_image_summaries():\n",
    "  image_elements = []\n",
    "  image_summaries = []\n",
    "  for i in os.listdir(output_path):\n",
    "    if i.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(output_path, i)\n",
    "        encoded_image = encode_image(image_path)\n",
    "        image_elements.append(encoded_image)\n",
    "        summary = summarize_image(encoded_image)\n",
    "        image_summaries.append(summary)\n",
    "  return image_elements, image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents(text_elements, text_summaries, table_elements, table_summaries):\n",
    "  # Create Documents and Vectorstore\n",
    "  documents = []\n",
    "  retrieve_contents = []\n",
    "\n",
    "  for e, s in zip(text_elements, text_summaries):\n",
    "      i = str(uuid.uuid4())\n",
    "      doc = Document(\n",
    "          page_content = s,\n",
    "          metadata = {\n",
    "              'id': i,\n",
    "              'type': 'text',\n",
    "              'original_content': e\n",
    "          }\n",
    "      )\n",
    "      retrieve_contents.append((i, e))\n",
    "      documents.append(doc)\n",
    "\n",
    "  for e, s in zip(table_elements, table_summaries):\n",
    "      doc = Document(\n",
    "          page_content = s,\n",
    "          metadata = {\n",
    "              'id': i,\n",
    "              'type': 'table',\n",
    "              'original_content': e\n",
    "          }\n",
    "      )\n",
    "      retrieve_contents.append((i, e))\n",
    "      documents.append(doc)\n",
    "\n",
    "  return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create a vectors store using Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "def vectorize_docs(docs):\n",
    "    url = \"http://localhost:6333\"\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        docs,\n",
    "        embeddings,\n",
    "        url=url,\n",
    "        prefer_grpc=False,\n",
    "        collection_name=\"UNIFI_Vector_DB\"\n",
    "    )\n",
    "    print(\"Vector DB Successfully Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = []\n",
    "for filename in os.listdir(\"../Data/Data Sources/Test/\"):\n",
    "  if filename.endswith('.pdf'):\n",
    "    pdf_files.append(filename)\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "documents_to_be_vectorized_ = []\n",
    "for pdf_file in tqdm(pdf_files):\n",
    "    try:\n",
    "        file_path = os.path.join('../Data/Data Sources/Test/', pdf_file)\n",
    "        raw_pdf_elements_ = get_elements_from_document(file_path)\n",
    "        text_elements_, text_summaries_, table_elements_, table_summaries_ = get_text_and_table_summaries(raw_pdf_elements_)\n",
    "        documents_ = create_documents(text_elements_, text_summaries_, table_elements_, table_summaries_)\n",
    "        documents_to_be_vectorized_.append(documents_)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for file: {pdf_file}\")\n",
    "        print(f\"Error message: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the elements in the documents list which is in the documents_to_be_vectorized list into a elements of one list of documents that will be vectorized\n",
    "all_documents = []\n",
    "for document in documents_to_be_vectorized_:\n",
    "    print(document)\n",
    "    all_documents.extend(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually go through the images extracted from the PDFs and remove redundant images before running the next 4 cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_summaries_(pdf_images_folder_path):\n",
    "  image_elements = []\n",
    "  image_summaries = []\n",
    "  for i in os.listdir(pdf_images_folder_path):\n",
    "    if i.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(pdf_images_folder_path, i)\n",
    "        encoded_image = encode_image(image_path)\n",
    "        image_elements.append(encoded_image)\n",
    "        summary = summarize_image(encoded_image)\n",
    "        image_summaries.append(summary)\n",
    "  return image_elements, image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_documents_(image_elements, image_summaries):\n",
    "  # Create Documents\n",
    "  documents = []\n",
    "  retrieve_contents = []\n",
    "\n",
    "  for e, s in zip(image_elements, image_summaries):\n",
    "      i = str(uuid.uuid4())\n",
    "      doc = Document(\n",
    "          page_content = s,\n",
    "          metadata = {\n",
    "              'id': i,\n",
    "              'type': 'image',\n",
    "              'original_content': e\n",
    "          }\n",
    "      )\n",
    "      retrieve_contents.append((i, s))\n",
    "      documents.append(doc)\n",
    "  return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the various folders in \"./images\" and get the summaries of the images\n",
    "documents_to_be_vectorized = []\n",
    "for folder in tqdm(sorted(os.listdir(output_path), reverse=True)):\n",
    "    if os.path.isdir(os.path.join(output_path, folder)):\n",
    "        pdf_images_folder_path = os.path.join(output_path, folder)\n",
    "        image_elements, image_summaries = get_image_summaries_(pdf_images_folder_path)\n",
    "        docs_ = create_image_documents_(image_elements, image_summaries)\n",
    "        documents_to_be_vectorized.append(docs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in documents_to_be_vectorized:\n",
    "    # print(document)\n",
    "    all_documents.extend(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the documents\n",
    "vectorize_docs(all_documents)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
