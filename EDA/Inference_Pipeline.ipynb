{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env file within your directory and add your OpenAI API key\n",
    "import openai, os\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"gpt-4-turbo-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use OpenAI's new text-embedding-3-large embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_db():\n",
    "    qdrant_client = QdrantClient(\n",
    "        url = \"http://localhost:6333\",\n",
    "        prefer_grpc=False\n",
    "    )\n",
    "    db = Qdrant(client=qdrant_client, embeddings=embeddings, collection_name=\"UNIFI_Vector_DB\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the integer value that corresponds to the value of the metric asked and nothing else.\n",
    "\"\"\"\n",
    "def set_custom_prompt():\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_qa_chain(llm, prompt, db):       \n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=db.as_retriever(search_kwargs={'k': 2}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt': prompt}\n",
    "                                       )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amkey = pd.read_csv('../AMKEY_GoldenStandard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amkey_synonyms = pd.read_csv('../ActivityMetricsSynonyms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_groups = [\"Absa\", \"Clicks\", \"Distell\", \"Oceana1&2\", \"Ssw\", \"Picknpay\", \"Impala\", \"Sasol\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_group_name_for_inference =  {\n",
    "    \"Absa\": \"Absa\",\n",
    "    \"Clicks\": \"Clicks Group Limited\",\n",
    "    \"Distell\": \"Distell\",\n",
    "    \"Oceana1&2\": \"Oceana Group\",\n",
    "    \"Ssw\": \"Sibanye Stillwater\",\n",
    "    \"Picknpay\": \"Pick n Pay\",\n",
    "    \"Impala\": \"Impala\",\n",
    "    \"Sasol\": \"Sasol\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_df = pd.DataFrame(columns=['ID', '2022_Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=llm_model, temperature=0.1)\n",
    "\n",
    "submission_iter = 0\n",
    "    \n",
    "vector_db_ = vector_db()\n",
    "\n",
    "for company_name in tqdm(company_groups):\n",
    "    qa_prompt = set_custom_prompt()\n",
    "    qa = retrieval_qa_chain(llm, qa_prompt, vector_db_)\n",
    "\n",
    "    # Iterate through each activity metric\n",
    "    for amkey_value, activity_metric in zip(amkey['AMKEY'], amkey['ActivityMetric']):\n",
    "        # Construct ID and query\n",
    "        company_or_group = company_name\n",
    "        # Use the amkey_value and company_or_group to get the ClientMetric amkey_synonyms dataframe\n",
    "        client_metric = \"\"\n",
    "        if not amkey_synonyms.empty:\n",
    "            client_metric_values = amkey_synonyms[(amkey_synonyms['AMKEY'] == amkey_value) & (amkey_synonyms['Group'] == company_or_group)]['ClientMetric'].values\n",
    "            if len(client_metric_values) > 0:\n",
    "                client_metric = client_metric_values[0]\n",
    "\n",
    "        if client_metric:\n",
    "            query = f\"What is the value of {client_metric} for {company_group_name_for_inference[company_or_group]} in 2022?\"\n",
    "        else:\n",
    "            query = f\"What is the value of {activity_metric} for {company_group_name_for_inference[company_or_group]} in 2022?\"\n",
    "        ID = f\"{amkey_value}_X_{company_or_group}\"\n",
    "\n",
    "        try:\n",
    "            # Query the model for the response\n",
    "            response = qa({'query': query})\n",
    "            print(response)\n",
    "            value = response['result']\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for company: {company_name} and activity metric: {activity_metric}\")\n",
    "            print(f\"Error message: {e}\")\n",
    "            value = 0.0\n",
    "\n",
    "        # Check if the ID exists in the sample_submission_df\n",
    "        if ID in sample_submission_df['ID'].values:\n",
    "            index = sample_submission_df[sample_submission_df['ID'] == ID].index[0]\n",
    "            existing_value = sample_submission_df.loc[index, '2022_Value']\n",
    "            if existing_value == \"I don't know.\" and value != \"I don't know.\":\n",
    "                # Update the value in sample_submission_df if it's not already set\n",
    "                sample_submission_df.loc[index, '2022_Value'] = value\n",
    "        else:\n",
    "            # Add a new entry to the sample_submission_df\n",
    "            sample_submission_df.loc[submission_iter] = [ID, value]\n",
    "            submission_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_df.to_csv('submission1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
